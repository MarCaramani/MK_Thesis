{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dbc1c84a-3a7f-4abb-a8ba-9f72ab0316b6",
   "metadata": {},
   "source": [
    "<span style=\"font-size: 24px; color:slateblue; text-decoration: underline; font-weight: bold;\">**QUESTIONS**</span>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b077eb9-30ba-42df-85fe-9c2a0d49d309",
   "metadata": {},
   "source": [
    "Installer: https://lamini-ai.github.io/Start/installer/  <font color=\"lime\">(includes system requirements)</font>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb9e99f6-d14a-416d-94e3-35d709e81635",
   "metadata": {},
   "source": [
    "Installation and Setup  https://lamini-ai.github.io/Start/setup/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61f3070a-b52c-4d88-b5d3-7f95c13d4ca7",
   "metadata": {},
   "source": [
    "Authentication https://lamini-ai.github.io/Start/auth/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ade02fe8-f5f1-4948-b29f-9f85ff51e289",
   "metadata": {},
   "source": [
    "Basic ModelRunner https://lamini-ai.github.io/runners/basic_model_runner/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5dcab9e-fe6c-418c-9f9a-9a729cdd3a87",
   "metadata": {},
   "source": [
    "ΈΧΕΙ ΠΟΛΥ ΠΛΗΡΟΦΟΡΙΑ ΑΠΟ ΠΟΥ ΝΑ ΞΕΚΙΝΗΣΩ ?????? \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e236df1-d69e-41ca-b802-8ab9ca77012c",
   "metadata": {},
   "source": [
    "Αρχικά πρέπει να βρεις τρόπο να έχεις προσβαση στο llma 2   \n",
    "https://ai.meta.com/blog/5-steps-to-getting-started-with-llama-2/?fbclid=IwAR26JMo0r91xEZbuB0LIPSHx5EhI_I1pDF8p-H9FI6RGdxnI8tnVm-3rEXM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a0e8031-e5bd-4c1a-b7fa-670ec2f40ca0",
   "metadata": {},
   "source": [
    "<font color= \"lime\" > ΑΓΝΟΗΣΕ ΤΙΣ ΠΑΡΑΠΑΝΩ ΕΡΩΤΗΣΕΙΣ </font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d5be7bc-609c-493a-b27c-0cd746f1c441",
   "metadata": {},
   "source": [
    "<font color=\"red\"> ΕΡΩΤΗΣΕΙΣ 25/12/2023 </font>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7436eddf-949f-4976-81c9-f655ef04a2f5",
   "metadata": {},
   "source": [
    "1. ΝΑ ΠΡΟΣΠΑΘΗΣΩ ΝΑ ΤΡΕΞΩ ΤΑ TRANSFORMERS ΣΤΟ JUPYTER LAB ? ΓΙΑΤΙ ΜΕΧΡΙ ΣΤΙΓΜΗΣ ΤΟ ΤΡΕΧΩ ΣΤΟ GOLAB ΤΗΣ GOOGLE."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b79a0b76-6aef-4488-9414-da32bbcdb7ac",
   "metadata": {},
   "source": [
    "2. ΠΡΕΠΕΙ ΝΑ ΕΧΩ ΕΝΕΡΓΟΠΟΙΗΜΕΝΟ ΤΟ CONDA ΓΙΑ ΝΑ ΚΑΝΩ GIT PUSH ? Η ΓΕΝΙΚΑ ΟΤΑΝ ΘΕΛΩ ΝΑ ΚΑΝΩ ΚΑΤΙ ΜΕ ΤΟ GIT ΠΡΕΠΕΙ ΝΑ ΕΙΝΑΙ ΕΝΕΡΓΟΠΟΙΗΜΕΝΟ ΤΟ CONDA? "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59ffa1e5-e935-459a-90e4-54d699303779",
   "metadata": {},
   "source": [
    "3.μπορει να υπάρχει συνδιασμός δυο μοντέλων ? δηλαδη να  γινει πρωτα ενα summarization και μετά το κείμενο που θα πάρουμε απο το summarization να ειναι το input μας για το question answers?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36af3b74-d5d4-4e3b-9555-032f4152f68c",
   "metadata": {},
   "source": [
    "ΕΡΩΤΗΣΕΙΣ 16/01/2023"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da753c91-af8b-40f3-bee6-4f0d87bcfd44",
   "metadata": {},
   "source": [
    "Έχω βρει τα παρακάτω μοντέλα τα οποια μου φαίνονται καλά για την διπλωματική αλλα θέλω και την γνώμη σου "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0056f036-0fe5-4f97-80c3-6cac462d4056",
   "metadata": {},
   "source": [
    "1. https://huggingface.co/deepset/tinyroberta-squad2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cb3f540-f096-4932-b4e7-11b460808f4d",
   "metadata": {},
   "source": [
    "2. https://huggingface.co/bert-large-uncased-whole-word-masking-finetuned-squad"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1a745a4-d6c8-4283-a6fd-dff693d36f2f",
   "metadata": {},
   "source": [
    "3.https://huggingface.co/deepset/roberta-base-squad2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d0b7e27-d71f-41b8-98cf-8a2c18e6c460",
   "metadata": {},
   "source": [
    "4. https://huggingface.co/phiyodr/bart-large-finetuned-squad2  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f95d368-79f4-4698-af22-603928a174cc",
   "metadata": {},
   "source": [
    "NOTES Υπάρχουν πολλά προ-εκπαιδευμένα μοντέλα για εργασίες NLP question answering που έχουν επιδείξει εξαιρετική απόδοση. Κάποια από τα δημοφιλή μοντέλα περιλαμβάνουν:\n",
    "\n",
    "1. **BERT (Bidirectional Encoder Representations from Transformers):** Το BERT είναι ένα από τα πρώτα προ-εκπαιδευμένα μοντέλα που έχουν επιτύχει σημαντική πρόοδο σε πολλές εργασίες NLP, συμπεριλαμβανομένης της Απάντησης σε Ερωτήσεις.\n",
    "\n",
    "2. **RoBERTa (Robustly optimized BERT approach):** Το RoBERTa είναι μια βελτιωμένη έκδοση του BERT που έχει πραγματοποιήσει αρκετές βελτιστοποιήσεις στην αρχιτεκτονική, προσφέροντας ακόμη καλύτερα αποτελέσματα.\n",
    "\n",
    "3. **GPT-3 (Generative Pre-trained Transformer 3):** Το GPT-3 είναι ένα από τα μεγαλύτερα και ισχυρότερα μοντέλα που έχουν κατασκευαστεί μέχρι στιγμής, χρησιμοποιώντας μεταγενέστερα μοντέλα Transformers. Μπορεί να χρησιμοποιηθεί και για ερωτήσεις-απαντήσεις.\n",
    "\n",
    "4. **DistilBERT:** Το DistilBERT είναι μια ελαφρύτερη έκδοση του BERT που διατηρεί υψηλή απόδοση με μικρότερο αριθμό παραμέτρων.\n",
    "\n",
    "5. **T5 (Text-to-Text Transfer Transformer):** Το T5 προτείνει να αντιμετωπίζετε όλες τις εργασίες NLP ως προβλήματα μετατροπής κειμένου σε κείμενο.\n",
    "\n",
    "6. **ALBERT (A Lite BERT for Self-supervised Learning of Language Representations):** Το ALBERT είναι μια άλλη ελαφρύτερη εκδοχή του BERT, σχεδιασμένη για μείωση του αριθμού των παραμέτρων χωρίς απώλεια στην απόδοση.\n",
    "\n",
    "Αυτά τα μοντέλα έχουν εκπαιδευτεί σε μεγάλα σύνολα δεδομένων και παρέχουν ισχυρές επιδόσεις σε ερωτήσεις-απαντήσεις. Η επιλογή του κατάλληλου μοντέλου εξαρτάται από τις συγκεκριμένες απαιτήσεις και τα δεδομένα της εφαρμογής σας."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e6c9e00-aff7-49fd-9182-ef6f94d68ebf",
   "metadata": {},
   "source": [
    "<font color=\"lightgreen\"> ΕΡΩΤΗΣΕΙΣ 04/02/2024 </font>\n",
    "\n",
    "1) Δεν μπορώ να βρω το μέγεθος που πρεπει να εχει το text και το summarization, νομιζω βασικα οτι το εχω βρει αλλα δεν ειμαι σιγουρη... είναι το max_position embeddings ?  το encoder ffn dim \n",
    "\n",
    "print(config)\n",
    "\n",
    "\n",
    "BartConfig {\n",
    "  \"activation_dropout\": 0.0,\n",
    "  \"activation_function\": \"gelu\",\n",
    "  \"attention_dropout\": 0.0,\n",
    "  \"bos_token_id\": 0,\n",
    "  \"classifier_dropout\": 0.0,\n",
    "  \"d_model\": 1024,\n",
    "  \"decoder_attention_heads\": 16,\n",
    "  \"decoder_ffn_dim\": 4096,\n",
    "  \"decoder_layerdrop\": 0.0,\n",
    "  \"decoder_layers\": 12,\n",
    "  \"decoder_start_token_id\": 2,\n",
    "  \"dropout\": 0.1,\n",
    "  \"encoder_attention_heads\": 16,\n",
    "  \"encoder_ffn_dim\": 4096,\n",
    "  \"encoder_layerdrop\": 0.0,\n",
    "  \"encoder_layers\": 12,\n",
    "  \"eos_token_id\": 2,\n",
    "  \"forced_eos_token_id\": 2,\n",
    "  \"id2label\": {\n",
    "    \"0\": \"LABEL_0\",\n",
    "    \"1\": \"LABEL_1\",\n",
    "    \"2\": \"LABEL_2\"\n",
    "  },\n",
    "  \"init_std\": 0.02,\n",
    "  \"is_encoder_decoder\": true,\n",
    "  \"label2id\": {\n",
    "    \"LABEL_0\": 0,\n",
    "    \"LABEL_1\": 1,\n",
    "    \"LABEL_2\": 2\n",
    "  },\n",
    "  \"max_position_embeddings\": 1024,\n",
    "  \"model_type\": \"bart\",\n",
    "  \"num_hidden_layers\": 12,\n",
    "  \"pad_token_id\": 1,\n",
    "  \"scale_embedding\": false,\n",
    "  \"transformers_version\": \"4.35.2\",\n",
    "  \"use_cache\": true,\n",
    "  \"vocab_size\": 50265\n",
    "}\n",
    "https://huggingface.co/docs/transformers/main/en/model_doc/bart#transformers.BartConfig\n",
    "https://huggingface.co/docs/transformers/main/en/tasks/summarization\n",
    "https://huggingface.co/docs/transformers/main/en/tasks/summarization\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1af01ac-2b67-4f71-b2c6-dfa84ab5a20e",
   "metadata": {},
   "source": [
    "2. Επισης τσεκαρε λιγο αν μπορεις το κομματι του κωδικα στο final code που αφορα το part 4 της προετοιμασιας των δεδομενων, δεν βγάζει κατι κοκκινο οταν το τρεχω οποτε αυτο ειναι καλο σημαδι αλλα απο την αλλη δεν βγαζει και κανενα print το οποιο με τρομαζει "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d99a298e-65e2-4928-a4b7-05ed17af0f58",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
