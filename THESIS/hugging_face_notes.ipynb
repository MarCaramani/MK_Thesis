{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "30949a0e-f40b-477b-85f5-9595bdfa1429",
   "metadata": {},
   "source": [
    "<span style=\"font-size: 24px; color:slateblue; text-decoration: underline; font-weight: bold;\">**Notes from the cources in hugging face**</span> \n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "e731e157-5230-45df-a044-3bd1f61ed6fe",
   "metadata": {},
   "source": [
    "<font  color=\"deepskyblue\"> Tokenizer </font>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc93d1e8-ed95-49a7-81c1-c108e96c5cbd",
   "metadata": {},
   "source": [
    "All this preprocessing needs to be done in exactly the same way as when the model was pretrained, so we first need to download that information from the Model Hub. To do this, we use the AutoTokenizer class and its from_pretrained() method. Using the checkpoint name of our model, it will automatically fetch the data associated with the modelâ€™s tokenizer and cache it (so itâ€™s only downloaded the first time you run the code below)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea1905af-6deb-4278-84b6-9abf4a0a73f0",
   "metadata": {},
   "source": [
    "  Model Hub > https://huggingface.co/models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1ec6217-b422-40f7-b1d1-296554fc7e3c",
   "metadata": {},
   "source": [
    "Other notes "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0f61e9e-2b93-4c9a-8d25-b0372f228b2c",
   "metadata": {},
   "source": [
    "We can download our pretrained model the same way we did with our tokenizer. ğŸ¤— Transformers provides an AutoModel class which also has a from_pretrained() method:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8916ad72-50c8-4808-b235-62ab2a29d980",
   "metadata": {},
   "source": [
    "The architecture contains only the base Transformer module  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81d174f2-c43b-472c-9b06-62b4004999e7",
   "metadata": {},
   "source": [
    "<font  color=\"deepskyblue\"> Tensors </font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5efc4734-382e-46b1-8b76-b9be9d6cbd07",
   "metadata": {},
   "source": [
    "To specify the type of tensors we want to get back (PyTorch, TensorFlow, or plain NumPy), we use the return_tensors argument:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1c76ccc-b4d7-4ce7-9f2a-e271427d9fbb",
   "metadata": {},
   "source": [
    "inputs = tokenizer(raw_inputs, padding=True, truncation=True, return_tensors=\"pt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64057794-8ded-4b50-955f-18c25d5b547f",
   "metadata": {},
   "source": [
    "return_tensors=\"pt\" ÏƒÎ·Î¼Î±Î¯Î½ÎµÎ¹ Î¿Ï„Î¹ Ï€ÎµÏÎ¹Î¼Î­Î½ÎµÎ¹ Î½Î± Ï€Î¬ÏÎµÎ¹ Ï„Î¿Ï…Ï‚ tensors Ï€Î¯ÏƒÏ‰ ÏƒÎµ Î¼ÏŒÏÏ†Î· Ï„Î·Ï‚ pytorch \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57aad8eb-c6bb-4e72-b6f1-938c9571b907",
   "metadata": {},
   "source": [
    "<font color=\"deepskyblue\"> Logits </font>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cd984bf-2c85-454e-864f-e4489b477602",
   "metadata": {},
   "source": [
    "\n",
    "Î¤Î± logits ÎµÎ¯Î½Î±Î¹ Î¿Î¹ Î±ÎºÎ±Ï„Î­ÏÎ³Î±ÏƒÏ„Î¿Î¹ Î²Î±Î¸Î¼Î¿Î»Î¿Î³Î¹ÎºÎ¿Î¯ Ï€ÏŒÎ½Ï„Î¿Î¹ Ï€Î¿Ï… Ï€Î±ÏÎ¬Î³ÎµÎ¹ Î­Î½Î± Î½ÎµÏ…ÏÏ‰Î½Î¹ÎºÏŒ Î´Î¯ÎºÏ„Ï…Î¿ Ï€ÏÎ¹Î½ Î±Ï€ÏŒ Ï„Î·Î½ ÎµÏ†Î±ÏÎ¼Î¿Î³Î® Ï„Î·Ï‚ ÏƒÏ…Î½Î¬ÏÏ„Î·ÏƒÎ·Ï‚ softmax. Î‘Ï…Ï„Î¿Î¯ Î¿Î¹ Î²Î±Î¸Î¼Î¿Î»Î¿Î³Î¹ÎºÎ¿Î¯ Ï€ÏŒÎ½Ï„Î¿Î¹ ÎµÎºÏ†ÏÎ¬Î¶Î¿Ï…Î½ Ï€ÏŒÏƒÎ¿ Ï€Î¹Î¸Î±Î½ÏŒ ÎµÎ¯Î½Î±Î¹ Î­Î½Î± Î´ÎµÎ´Î¿Î¼Î­Î½Î¿ Î´ÎµÎ¯Î³Î¼Î± Î½Î± Î±Î½Î®ÎºÎµÎ¹ ÏƒÎµ ÎºÎ¬Î¸Îµ Î´Ï…Î½Î±Ï„Î® ÎºÎ±Ï„Î·Î³Î¿ÏÎ¯Î± Î® ÎºÎ»Î¬ÏƒÎ·. ÎŸÏ…ÏƒÎ¹Î±ÏƒÏ„Î¹ÎºÎ¬, Ï„Î± logits Î±Ï€Î¿Ï„ÎµÎ»Î¿ÏÎ½ Ï„Î·Î½ Î±Î½Ï„Î¯Î´ÏÎ±ÏƒÎ· Ï„Î¿Ï… Î¼Î¿Î½Ï„Î­Î»Î¿Ï… Ï€ÏÎ¹Î½ Ï„Î·Î½ \"Î±Ï€ÏŒÏ†Î±ÏƒÎ·\" Ï„ÎµÎ»Î¹ÎºÎ®Ï‚ ÎºÎ±Ï„Î·Î³Î¿ÏÎ¯Î±Ï‚.\n",
    "\n",
    "Î— Î¼ÎµÏ„Î±Ï„ÏÎ¿Ï€Î® Ï„Ï‰Î½ logits ÏƒÎµ Ï€Î¹Î¸Î±Î½ÏŒÏ„Î·Ï„ÎµÏ‚ ÎµÏ€Î¹Ï„Ï…Î³Ï‡Î¬Î½ÎµÏ„Î±Î¹ ÏƒÏ…Î½Î®Î¸Ï‰Ï‚ Î¼Îµ Ï„Î· Ï‡ÏÎ®ÏƒÎ· Ï„Î·Ï‚ ÏƒÏ…Î½Î¬ÏÏ„Î·ÏƒÎ·Ï‚ softmax. Î— softmax Ï€Î±Î¯ÏÎ½ÎµÎ¹ Î­Î½Î±Î½ Î´Î¹Î¬Î½Ï…ÏƒÎ¼Î± logits ÎºÎ±Î¹ ÎµÏ€Î¹ÏƒÏ„ÏÎ­Ï†ÎµÎ¹ Î­Î½Î± Î´Î¹Î¬Î½Ï…ÏƒÎ¼Î± Ï€Î¹Î¸Î±Î½Î¿Ï„Î®Ï„Ï‰Î½, ÎµÎ¾Î±ÏƒÏ†Î±Î»Î¯Î¶Î¿Î½Ï„Î±Ï‚ ÏŒÏ„Î¹ Î¿Î¹ Ï€Î¹Î¸Î±Î½ÏŒÏ„Î·Ï„ÎµÏ‚ Î±Î¸ÏÎ¿Î¯Î¶Î¿Î½Ï„Î±Î¹ ÏƒÎµ 1. Î‘Ï…Ï„ÏŒ ÎµÎ¯Î½Î±Î¹ Ï‡ÏÎ®ÏƒÎ¹Î¼Î¿ ÏƒÎµ Ï€ÏÎ¿Î²Î»Î®Î¼Î±Ï„Î± ÎºÎ±Ï„Î·Î³Î¿ÏÎ¹Î¿Ï€Î¿Î¯Î·ÏƒÎ·Ï‚, ÏŒÏ€Î¿Ï… Î¸Î­Î»Î¿Ï…Î¼Îµ Î½Î± Î±Ï€Î¿Ï†Î±ÏƒÎ¯ÏƒÎ¿Ï…Î¼Îµ ÏƒÎµ Ï€Î¿Î¹Î± ÎºÎ±Ï„Î·Î³Î¿ÏÎ¯Î± Î±Î½Î®ÎºÎµÎ¹ Î­Î½Î± Î´ÎµÎ¯Î³Î¼Î± Î²Î¬ÏƒÎµÎ¹ Ï„Ï‰Î½ Î±Î½Ï„Î¯ÏƒÏ„Î¿Î¹Ï‡Ï‰Î½ Ï€Î¹Î¸Î±Î½Î¿Ï„Î®Ï„Ï‰Î½."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6af33cb-434c-4610-b406-a84bdc1847bf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
